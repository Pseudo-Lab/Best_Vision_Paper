<h1 align="center"> Best Vision Paper </h1>

<div align="center">
<a href="https://discord.gg/EPurkHVtp2"><img src="https://img.shields.io/badge/Discord-BF40BF" alt="Discord Community"/></a>
<a href="https://github.com/Pseudo-Lab/Best_Vision_Paper/stargazers"><img src="https://img.shields.io/github/stars/Pseudo-Lab/Best_Vision_Paper" alt="Stars Badge"/></a>
<a href="https://github.com/Pseudo-Lab/Best_Vision_Paper/network/members"><img src="https://img.shields.io/github/forks/Pseudo-Lab/Best_Vision_Paper" alt="Forks Badge"/></a>
<a href="https://github.com/Pseudo-Lab/Best_Vision_Paper/pulls"><img src="https://img.shields.io/github/issues-pr/Pseudo-Lab/Best_Vision_Paper" alt="Pull Requests Badge"/></a>
<a href="https://github.com/Pseudo-Lab/Best_Vision_Paper/issues"><img src="https://img.shields.io/github/issues/Pseudo-Lab/Best_Vision_Paper" alt="Issues Badge"/></a>
<a href="https://github.com/Pseudo-Lab/Best_Vision_Paper/graphs/contributors"><img alt="GitHub contributors" src="https://img.shields.io/github/contributors/Pseudo-Lab/Best_Vision_Paper?color=2b9348"></a>
<a href="https://hits.seeyoufarm.com"><img src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2FPseudo-Lab%2FBest_Vision_Paper&count_bg=%2379C83D&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=hits&edge_flat=false"/></a>
</div>
<br>

<!-- sheilds: https://shields.io/ -->
<!-- hits badge: https://hits.seeyoufarm.com/ -->

> Welcome to the Best Vision Paper Team Repository.
>
> We review and analyze the best papers from top-tier vision conferences including CVPR, ICCV, ECCV, and NeurIPS and more.
>
> Our team dives deep into these groundbreaking papers to understand their novel approaches and technical innovations.
> These award-winning papers represent the pinnacle of computer vision research, offering crucial insights into where the field is heading.
> Through careful analysis and discussion, we aim to understand not just the technical details, but also the key insights that drive the field forward.
> 
> What we do:
> - **Comprehensive Reviews** for in-depth best vision paper analysis
> - Weekly discussions to **share insights** and **explore potential research directions**
> - Technical deep-dives including code analysis and implementation details when available
>   
> Join us in advancing the field of computer vision through open collaboration and innovation!

<br>

> 저희 팀은 스터디를 통해 최고 수준의 비전 컨퍼런스에서 수상한 논문들을 심층 분석하고, 도출한 아이디어와 인사이트를 공유하는 것을 목표로 합니다.
> 
> 수상 경력에 빛나는 논문들은 컴퓨터 비전 연구의 정점을 나타내며, 비전 분야가 나아갈 방향에 대한 중요한 인사이트를 제공합니다.
> 
> 따라서 심층적 토론을 통해 비전 연구의 미래를 탐색 및 대응하고 연구 역량 강화를 통한 성장을 추구하는 분을 모시고자 합니다.
> 
> 논문은 2년 이내(2023~)의 최고 비전 컨퍼런스를 기준으로 자율적으로 선정 및 발표를 진행할 예정입니다.(나머지 Github 참조)

<br>

## 🌟 프로젝트 목표 (Project Vision)
_"다양한 비전(Vision) 논문을 심층 분석하고, 공유와 협업을 통해 새로운 통찰을 얻기"_  
- **최신 비전 연구 동향 탐색**을 통해 **빠르게 변화하는 비전 분야의 핵심 트렌드**에 대응
- **심층적 논문 분석** 및 **인사이트 도출**을 통한 연구 역량 강화
- **협업 기반의 지식 공유**를 통한 오픈소스 정신 지향
- **개인 역량 강화** 및 **네트워킹 확장**을 통한 미래 기회 모색하기

<br>

## 논문 선정 
- 2년 이내 Top-Tier Vision Conference에서 선정된 Best (Student) paper, Spotlight 혹은 이에 준하는 논문
  - 대상 학회: CVPR, ICCV, ICLR, ICML, ECCV, NeurIPS, AAAI 등 Top-tier Vision 학회
  - 참고 자료
      - [Best Papers Top Venues](https://github.com/SarahRastegar/Best-Papers-Top-Venues)
      - [CVPR 2024 Best Paper Award Winners](https://www.computer.org/press-room/cvpr-2024-announces-best-paper-award-winners)
    
- 시의성이 큰 Tech Report 
  - 예시: 2025 CES NVIDIA Cosmos, BigTech Tech Report

개인의 선호에 맞게 자율적으로 선택 및 발표 진행

<br>

## 발표자 진행 안내
- 논문 고지 : 발표일로부터 **최소 2주 전** 발표할 논문을 내부 **Discord에 사전 고지** & **Github 주차별 활동에 기입**
- 발표 자료 : 자율적으로 구성 및 작성
- 발표 후 : github에 자료 공유
<br>

## 파일 공유 방법

- 방법 1. 진행한 발표자료를 1) "Github Repository 내 upload" 혹은 2) "클라우드 내 upload" 진행. (가급적 Repository upload 요청)
  - "Github Repository 내 upload" 진행시 : Conference별 연도별 파일 upload 진행 요청.
  - "클라우드 내 upload" 진행시 : 전체 공유가 가능한 링크 준비 요청.
- 방법 2. upload한 발표자료 링크를 기반으로 presentations 폴더 내에 존재하는 [conference_upload_list.md](https://github.com/Pseudo-Lab/Best_Vision_Paper/blob/main/presentations/conference_upload_list.md) 파일에 기입.

e.g. 
CVPR의 경우 해당 목차 생성 후 아래와 같은 Table 구성 진행함.

| Paper Title | Award | Project Page / Arxiv Page | Presenter | URL | 
| --------| -------- | -------- | -------- | -------- |
| 2024 |||||
| Generative Image Dynamics | CVPR 2024, Best Paper Award | [Project](https://generative-dynamics.github.io/) / [Arxiv](https://arxiv.org/abs/2309.07906) | Geonhak Song | [presenter_file](https://github.com/Pseudo-Lab/Best_Vision_Paper/blob/main/presentations/CVPR/2024/%5BCVPR%202024%5D%20Generative%20Image%20Dynamics%20(Best%20Paper%20Award).pdf) |

<br>

## 🌱 참여 안내 (How to Engage)

**누구나 청강을 통해 모임을 참여하실 수 있습니다.**  
- 특별한 신청 없이 매주 화요일 오후 9:00~10:30에 맞추어 디스코드 #Room-AT 채널로 입장
- 1기 : 2025.03.04 ~ 2025.06.24

<br>

## 🧑 Contributor 

| 역할          | 이름 |  관심 분야                                                               | LinkedIn                         |
|---------------|------|-----------------------------------------------------------------------|----------------------------------------|
| **Project Manager** | 송건학 | Vision 기반 Generative AI (2D, 3D etc), Agent | [LinkedIN](https://www.linkedin.com/in/geonhak-song-09a037165/) |
| **Member** | 고재훈 | Multi-modal Learning, 3D Vision | [LinkedIN](https://www.linkedin.com/in/jaehoon2123/) |
| **Member** | 공성택 |  |                |
| **Member** | 김명섭 | Medical AI, Generative AI (2D, 3D etc) | [LinkedIN](https://www.linkedin.com/in/kmscopra/) |
| **Member** | 김지환 | Generative Model, Segmentation (3D Medical image) | [LinkedIN](https://www.linkedin.com/in/kuchoco97/) |
| **Member** | 채진영 | Multi-modal learning, Knowledge-based reasoning, Agent | [LinkedIN](https://www.linkedin.com/in/jinyeong-chae419) |

<br>

## 🚀 Best Vision Paper Team 로드맵 (Roadmap)
```mermaid
gantt
    title 2025 Best Vision Paper Team 여정
    section 일정    
    OT & cycle       :a1, 2025-03-04, 96d    
    section 가짜연구소 활동
    Magical Week1: 2025-03-23, 7d
    Magical Week2: 2025-04-27, 7d
    휴일주차 (05.06): holiday, 2025-05-04, 7d
    PseudoCon 2025     : conference, 2025-05-17, 1d
    
```

<br>

## 💻 주차별 활동 (Activity History)

|회차| 날짜 | 논문 이름 | 학회 : 수상 이력 | Project/논문 링크 | 발표자 | 
| --------| -------- | -------- | -------- | -------- | ---- |
|1| 2025/03/04 | OT ||| 송건학 | 
|2| 2025/03/11 | Generative Image Dynamics | CVPR 2024, Best Paper Award | [Project](https://generative-dynamics.github.io/) / [Arxiv](https://arxiv.org/abs/2309.07906) | 송건학 | 
|3| 2025/03/18 | Is ImageNet worth 1 video? Learning strong image encoders from 1 long unlabelled video | ICLR 2024, Honorable Mention |[Project](https://shashankvkt.github.io/dora) / [Arxiv](https://arxiv.org/abs/2309.07906) | 고재훈 | 
|4| 2025/03/25 | Magical Week ||| | 
|5| 2025/04/01 | PixelSplat: 3D Gaussian Splats from Image Pairs for Scalable Generalizable 3D Reconstruction | CVPR 2024, Honorable Mention |[Project](https://davidcharatan.com/pixelsplat/) / [Arxiv](https://arxiv.org/abs/2312.12337)| 공성택 | 
|6| 2025/04/08 | Protein Discovery with Discrete Walk-Jump Sampling | ICLR 2024, Outstanding Papers | [Arxiv](https://arxiv.org/abs/2306.12360) | 김지환 | 
|7| 2025/04/15 | Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction | NeurIPS 2024, Best Paper |[Project](https://github.com/FoundationVision/VAR) / [Arxiv](https://arxiv.org/pdf/2404.02905) | 채진영 | 
|8| 2025/04/22 | Rich Human Feedback for Text-to-Image Generation | CVPR 2024, Best Paper Award | [Project](https://github.com/google-research/google-research/tree/master/richhf_18k) / [Arxiv](https://arxiv.org/abs/2312.10240) | 김명섭 | 
|9| 2025/04/29 | Magical Week |||  | 
|10| 2025/05/06 | 대체 휴일 ||| | 
|11| 2025/05/13 | Reliable Conflictive Multi-view Learning | AAAI 2024, Outstanding Paper Award | [Project](https://github.com/jiajunsi/RCML) / [Arxiv](https://arxiv.org/abs/2402.16897) | 고재훈 | 
|12| 2025/05/20 | TBD ||| TBD (1명) | 
|13| 2025/05/27 | TBD ||| 김지환 | 
|14| 2025/06/03 | 대선 휴일 ||| | 
|15| 2025/06/10 | TBD ||| 송건학 | 
|16| 2025/06/17 | TBD ||| 채진영 | 
|17| 2025/06/24 | TBD ||| TBD (1명) | 
|18| 2025/07/01 | TBD ||| TBD (1명) | 


<br>

## Acknowledgement 🙏

Best Vision Paper Team is developed as part of Pseudo-Lab's Open Research Initiative. Special thanks to our contributors and the open source community for their valuable insights and contributions.

## About Pseudo Lab 👋🏼</h2>

[Pseudo-Lab](https://pseudo-lab.com/) is a non-profit organization focused on advancing machine learning and AI technologies. Our core values of Sharing, Motivation, and Collaborative Joy drive us to create impactful open-source projects. With over 5k+ researchers, we are committed to advancing machine learning and AI technologies.

<h2>Contributors 😃</h2>
<a href="https://github.com/Pseudo-Lab/Best_Vision_Paper/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=Pseudo-Lab/Best_Vision_Paper" />
</a>
<br><br>

<h2>License 🗞</h2>

This project is licensed under the [MIT License](https://opensource.org/licenses/MIT).
